{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfd8b73",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93f957",
   "metadata": {},
   "source": [
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch.\n",
    "\n",
    "Web scraping is used to scrape the data from different websites and glean actionable intelligence from these sites in terms of equity research.It is also used to automate the data extraction and competitive analysis.\n",
    "\n",
    "Three areas where Web Scraping is used to get data are as:\n",
    "\n",
    "1. Price Monitoring\n",
    "2. Market Reasearch\n",
    "3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12f1fb",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ab73b",
   "metadata": {},
   "source": [
    "Different methods used for Web Scraping are:\n",
    "\n",
    "1. Manual Scraping\n",
    "   Not surprisingly, web scraping can be done manually. All you need is the         ability to copy/paste information and a spreadsheet to keep track of the       extracted data.\n",
    "   \n",
    "\n",
    "2. Automated Scraping\n",
    "   On the other side of the spectrum, we have automated web scraping tools        known as web scrappers.\n",
    "\n",
    "   Automated web scraping tools have become increasingly popular due their ease    of use and savings in time and costs.\n",
    "\n",
    "   These tools also come in many different shapes and sizes, from simple          browser extensions to more powerful software solutions.\n",
    "\n",
    "   Some Examples are ParseHub Scraper,Scraper API etc.\n",
    "\n",
    "\n",
    "\n",
    "3. Outsourced Web Scraping\n",
    "\n",
    "   After all, you might not want to bother with managing your web scraping       projects yourself. In this scenario, you might decide to outsource your web     scraping projects.\n",
    "\n",
    "  In this case, you must be vary of the methods used by the contractor you have   selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dacf2e5",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4d954",
   "metadata": {},
   "source": [
    "BeautifulSoup is another Python library, commonly used to parse data from XML and HTML documents. Organizing this parsed content into more accessible trees, BeautifulSoup makes navigating and searching through large swathes of data much easier. It’s the go-to tool for many data analysts.\n",
    "\n",
    "It is used to transform a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, so you don't have to think about encodings.It is basically used to improve the readability of the scrapped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a5585",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d57d5",
   "metadata": {},
   "source": [
    "Flask is a fast ,lightweight and Beginners friendly web Framework written in Python which uses Werkzeug WSGI toolkit and Jinja2 template engine. Starter code to write the “Hello World” Program in Flask is very sample.\n",
    "\n",
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape.\n",
    "\n",
    "\n",
    "from flask import Flask, render_template\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "The first line imports the Flask class and the render_template method from the flask library.The second line imports the BeautifulSoup class, and the third line imports the requests module from our Python library.\n",
    "\n",
    "Flask used in this Web Scraping project as it provides inbluilt modules,libraries and methods to read and parse the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93ac36",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d25caa",
   "metadata": {},
   "source": [
    "The AWS services used in this project are:\n",
    "\n",
    "1. CodePipeline: AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously. \n",
    "\n",
    "You can use CodePipeline to help you automatically build, test, and deploy your applications in the cloud. Specifically, you can:\n",
    "\n",
    "- Automate your release processes: CodePipeline fully automates your release process from end to end, starting from your source repository through build, test, and deployment. You can prevent changes from moving through a pipeline by including a manual approval action in any stage except a Source stage. You can release when you want, in the way you want, on the systems of your choice, across one instance or multiple instances.\n",
    "\n",
    "- Establish a consistent release process: Define a consistent set of steps for every code change. CodePipeline runs each stage of your release according to your criteria.\n",
    "\n",
    "- Speed up delivery while improving quality: You can automate your release process to allow your developers to test and release code incrementally and speed up the release of new features to your customers.\n",
    "\n",
    "- Use your favorite tools: You can incorporate your existing source, build, and deployment tools into your pipeline. For a full list of AWS services and third-party tools currently supported by CodePipeline, see Product and service integrations with CodePipeline.\n",
    "\n",
    "- View progress at a glance: You can review real-time status of your pipelines, check the details of any alerts, retry failed actions, view details about the source revisions used in the latest pipeline execution in each stage, and manually rerun any pipeline.\n",
    "\n",
    "- View pipeline history details: You can view details about executions of a pipeline, including start and end times, run duration, and execution IDs.\n",
    "\n",
    "2. Elastic Beanstalk: AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and AWS Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\n",
    "\n",
    "First of all github is connencted with the CodePipeline then it is connencted to the Elastic Beanstalk which is like a environment setup whiich provides the memory,storage cpu and environment that is needed for the deployment of the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
